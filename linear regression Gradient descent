import numpy as np
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Input data
X = np.asarray([20, 5, 10, 13, 12])
Y = np.asarray([44, 22, 25, 32, 27])

# Initialize model parameters
b1 = 0
b0 = 0

L = 0.001  # Learning rate
epochs = 1000  # Number of iterations for gradient descent

n = np.size(X)  # Number of elements in X

# Performing Gradient Descent
for i in range(epochs):
    Total_Err_x, Total_Err_y = 0, 0  # Resetting the error values for each epoch
    for x, y in zip(X, Y):
        y_pred = b1 * x + b0  # The current predicted value of y
        err = y - y_pred  # The error
        Total_Err_x += x * err  # Gradient with respect to b1
        Total_Err_y += err  # Gradient with respect to b0

    # Calculate the gradients
    D_b1 = (-2 / n) * Total_Err_x  # Derivative with respect to b1
    D_b0 = (-2 / n) * Total_Err_y  # Derivative with respect to b0

    # Update parameters
    b1 -= L * D_b1
    b0 -= L * D_b0

    # Optionally, print progress every 100 epochs
    if i % 100 == 0:
        print(f"Epoch {i}: b1 = {b1}, b0 = {b0}")

# Make predictions
Y_pred = b1 * X + b0

# Calculate MSE and R-squared
MSE = mean_squared_error(Y, Y_pred)
coefficient_of_determination = r2_score(Y, Y_pred)

# Print MSE and R-squared
print('MSE: %f' % MSE)
print('Coefficient of determination (R-squared): %f' % coefficient_of_determination)

# Visualize the results
plt.figure(figsize=(10, 5))
plt.scatter(X, Y, color='blue', label='Actual data')
plt.plot(X, Y_pred, 'r', linewidth=2, label='Fitted line')
plt.title("Linear Regression using Gradient Descent")
plt.xlabel("X")
plt.ylabel("Y")
plt.legend()
plt.show()
